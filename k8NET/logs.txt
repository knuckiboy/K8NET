
==> Audit <==
|-----------|-------------------|----------|-------------|---------|---------------------|---------------------|
|  Command  |       Args        | Profile  |    User     | Version |     Start Time      |      End Time       |
|-----------|-------------------|----------|-------------|---------|---------------------|---------------------|
| start     |                   | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 10:28 +08 | 15 May 24 10:33 +08 |
| dashboard |                   | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 10:39 +08 |                     |
| kubectl   | cluster-info      | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 10:43 +08 | 15 May 24 10:43 +08 |
| kubectl   | cluster-info      | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 10:43 +08 | 15 May 24 10:43 +08 |
| kubectl   | get namespaces    | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 10:44 +08 | 15 May 24 10:44 +08 |
| kubectl   | get namespaces    | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 15:06 +08 | 15 May 24 15:06 +08 |
| tunnel    |                   | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 15:12 +08 |                     |
| tunnel    |                   | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 15:18 +08 |                     |
| tunnel    |                   | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 15:19 +08 |                     |
| service   | k8net-web-service | minikube | JNNBOOK\Jan | v1.33.0 | 15 May 24 15:21 +08 |                     |
|-----------|-------------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/15 10:28:35
Running on machine: JNNBOOK
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0515 10:28:35.587734    7792 out.go:291] Setting OutFile to fd 112 ...
I0515 10:28:35.588292    7792 out.go:304] Setting ErrFile to fd 116...
W0515 10:28:35.603107    7792 root.go:314] Error reading config file at C:\Users\Jan\.minikube\config\config.json: open C:\Users\Jan\.minikube\config\config.json: The system cannot find the path specified.
I0515 10:28:35.612674    7792 out.go:298] Setting JSON to false
I0515 10:28:35.617983    7792 start.go:129] hostinfo: {"hostname":"JNNBOOK","uptime":493,"bootTime":1715739621,"procs":262,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3593 Build 22631.3593","kernelVersion":"10.0.22631.3593 Build 22631.3593","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"ae83cfdb-e9e6-424d-a134-6f2cd3e76e6b"}
W0515 10:28:35.617983    7792 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0515 10:28:35.621584    7792 out.go:177] ðŸ˜„  minikube v1.33.0 on Microsoft Windows 11 Home 10.0.22631.3593 Build 22631.3593
I0515 10:28:35.623917    7792 notify.go:220] Checking for updates...
W0515 10:28:35.624955    7792 preload.go:294] Failed to list preload files: open C:\Users\Jan\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0515 10:28:35.625720    7792 driver.go:392] Setting default libvirt URI to qemu:///system
I0515 10:28:35.626230    7792 global.go:112] Querying for installed drivers using PATH=C:\Program Files\Microsoft SDKs\Azure\CLI2\wbin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\ProgramData\chocolatey\bin;C:\Program Files\Microsoft VS Code\bin;C:\Program Files\Azure Data Studio\bin;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files\Amazon\AWSCLIV2\;C:\Program Files (x86)\Microsoft SQL Server\160\Tools\Binn\;C:\Program Files\Microsoft SQL Server\160\Tools\Binn\;C:\Program Files\Microsoft SQL Server\160\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\160\DTS\Binn\;C:\Program Files\dotnet\;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Git\cmd;C:\Program Files\nodejs\;C:\Users\Jan\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Azure Data Studio\bin;C:\Users\Jan\AppData\Local\GitHubDesktop\bin;C:\Users\Jan\.dotnet\tools;C:\Users\Jan\AppData\Roaming\npm
I0515 10:28:35.746417    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\last_update_check: {Name:mkb6ee61019e7ac603e83f44a2836f692643a1e2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:28:35.748180    7792 out.go:177] ðŸŽ‰  minikube 1.33.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.1
I0515 10:28:35.753310    7792 out.go:177] ðŸ’¡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0515 10:28:35.823367    7792 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0515 10:28:35.830182    7792 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0515 10:28:38.527799    7792 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.6976166s)
I0515 10:28:38.528329    7792 info.go:266] docker info: {ID:9b80fe5c-5c0f-4b13-bcdc-23c5df533528 Containers:4 ContainersRunning:2 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:62 OomKillDisable:true NGoroutines:81 SystemTime:2024-05-15 02:28:38.500173732 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6084681728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0515 10:28:38.528329    7792 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0515 10:28:38.536649    7792 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0515 10:28:38.536649    7792 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0515 10:28:43.668583    7792 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0515 10:28:43.677628    7792 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0515 10:28:43.694665    7792 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0515 10:28:43.703211    7792 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0515 10:28:43.703211    7792 driver.go:314] not recommending "ssh" due to default: false
I0515 10:28:43.703211    7792 driver.go:349] Picked: docker
I0515 10:28:43.703211    7792 driver.go:350] Alternatives: [hyperv ssh]
I0515 10:28:43.703211    7792 driver.go:351] Rejects: [podman qemu2 virtualbox vmware]
I0515 10:28:43.704819    7792 out.go:177] âœ¨  Automatically selected the docker driver. Other choices: hyperv, ssh
I0515 10:28:43.707292    7792 start.go:297] selected driver: docker
I0515 10:28:43.707292    7792 start.go:901] validating driver "docker" against <nil>
I0515 10:28:43.707292    7792 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0515 10:28:43.719702    7792 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0515 10:28:44.100044    7792 info.go:266] docker info: {ID:9b80fe5c-5c0f-4b13-bcdc-23c5df533528 Containers:4 ContainersRunning:2 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:58 OomKillDisable:true NGoroutines:73 SystemTime:2024-05-15 02:28:44.071970155 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6084681728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0515 10:28:44.100587    7792 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0515 10:28:44.133880    7792 start_flags.go:393] Using suggested 3000MB memory alloc based on sys=12030MB, container=5802MB
I0515 10:28:44.134387    7792 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0515 10:28:44.135447    7792 out.go:177] ðŸ“Œ  Using Docker Desktop driver with root privileges
I0515 10:28:44.136383    7792 cni.go:84] Creating CNI manager for ""
I0515 10:28:44.136383    7792 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 10:28:44.136383    7792 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0515 10:28:44.136383    7792 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Jan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0515 10:28:44.137443    7792 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0515 10:28:44.137970    7792 cache.go:121] Beginning downloading kic base image for docker with docker
I0515 10:28:44.139016    7792 out.go:177] ðŸšœ  Pulling base image v0.0.43 ...
I0515 10:28:44.140070    7792 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0515 10:28:44.140070    7792 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0515 10:28:44.287296    7792 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0515 10:28:44.287823    7792 localpath.go:146] windows sanitize: C:\Users\Jan\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Jan\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0515 10:28:44.287823    7792 localpath.go:146] windows sanitize: C:\Users\Jan\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Jan\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0515 10:28:44.287823    7792 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local cache directory
I0515 10:28:44.288875    7792 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0515 10:28:44.324965    7792 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0515 10:28:44.324965    7792 cache.go:56] Caching tarball of preloaded images
I0515 10:28:44.324965    7792 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0515 10:28:44.325998    7792 out.go:177] ðŸ’¾  Downloading Kubernetes v1.30.0 preload ...
I0515 10:28:44.327051    7792 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0515 10:28:44.536178    7792 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4?checksum=md5:00b6acf85a82438f3897c0a6fafdcee7 -> C:\Users\Jan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0515 10:30:20.522442    7792 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0515 10:30:20.522442    7792 preload.go:255] verifying checksum of C:\Users\Jan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0515 10:30:21.362917    7792 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0515 10:30:21.364575    7792 profile.go:143] Saving config to C:\Users\Jan\.minikube\profiles\minikube\config.json ...
I0515 10:30:21.364575    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\config.json: {Name:mk8fa4d70aa76be466a28cbf63d1575978f7d5ed Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:30:53.241385    7792 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 as a tarball
I0515 10:30:53.241385    7792 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from local cache
I0515 10:30:53.241385    7792 localpath.go:146] windows sanitize: C:\Users\Jan\.minikube\cache\kic\amd64\kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar -> C:\Users\Jan\.minikube\cache\kic\amd64\kicbase_v0.0.43@sha256_7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737.tar
I0515 10:31:27.546697    7792 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from cached tarball
I0515 10:31:27.548813    7792 cache.go:194] Successfully downloaded all kic artifacts
I0515 10:31:27.591978    7792 start.go:360] acquireMachinesLock for minikube: {Name:mkaf5d97561daf3a52eb34cc9c5e3b77f74c7c72 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0515 10:31:27.591978    7792 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0515 10:31:27.596842    7792 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Jan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0515 10:31:27.599311    7792 start.go:125] createHost starting for "" (driver="docker")
I0515 10:31:27.607833    7792 out.go:204] ðŸ”¥  Creating docker container (CPUs=2, Memory=3000MB) ...
I0515 10:31:27.640250    7792 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0515 10:31:27.640250    7792 client.go:168] LocalClient.Create starting
I0515 10:31:27.643558    7792 main.go:141] libmachine: Creating CA: C:\Users\Jan\.minikube\certs\ca.pem
I0515 10:31:28.069873    7792 main.go:141] libmachine: Creating client certificate: C:\Users\Jan\.minikube\certs\cert.pem
I0515 10:31:29.443452    7792 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0515 10:31:29.623821    7792 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0515 10:31:29.633445    7792 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0515 10:31:29.633445    7792 cli_runner.go:164] Run: docker network inspect minikube
W0515 10:31:29.826067    7792 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0515 10:31:29.826067    7792 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0515 10:31:29.826067    7792 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0515 10:31:29.833507    7792 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0515 10:31:30.115554    7792 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc002bdb8f0}
I0515 10:31:30.116087    7792 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0515 10:31:30.122546    7792 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0515 10:31:30.384166    7792 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0515 10:31:30.384702    7792 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0515 10:31:30.397677    7792 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0515 10:31:30.652181    7792 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0515 10:31:31.026622    7792 oci.go:103] Successfully created a docker volume minikube
I0515 10:31:31.034252    7792 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0515 10:31:35.060218    7792 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (4.0259655s)
I0515 10:31:35.060218    7792 oci.go:107] Successfully prepared a docker volume minikube
I0515 10:31:35.060773    7792 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0515 10:31:35.060773    7792 kic.go:194] Starting extracting preloaded images to volume ...
I0515 10:31:35.069056    7792 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Jan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0515 10:32:07.441429    7792 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Jan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (32.3723728s)
I0515 10:32:07.441429    7792 kic.go:203] duration metric: took 32.3806556s to extract preloaded images to volume ...
I0515 10:32:07.456387    7792 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0515 10:32:08.581278    7792 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.1248907s)
I0515 10:32:08.581816    7792 info.go:266] docker info: {ID:9b80fe5c-5c0f-4b13-bcdc-23c5df533528 Containers:4 ContainersRunning:2 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:true NGoroutines:83 SystemTime:2024-05-15 02:32:08.462000925 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6084681728 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0515 10:32:08.592699    7792 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0515 10:32:09.496360    7792 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0515 10:32:12.136041    7792 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737: (2.6396802s)
I0515 10:32:12.146162    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0515 10:32:12.549770    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:32:13.248656    7792 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0515 10:32:14.341710    7792 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.0930537s)
I0515 10:32:14.341710    7792 oci.go:144] the created container "minikube" has a running status.
I0515 10:32:14.342226    7792 kic.go:225] Creating ssh key for kic: C:\Users\Jan\.minikube\machines\minikube\id_rsa...
I0515 10:32:14.951888    7792 kic_runner.go:191] docker (temp): C:\Users\Jan\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0515 10:32:15.971894    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:32:17.801960    7792 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.8300665s)
I0515 10:32:17.881604    7792 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0515 10:32:17.881604    7792 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0515 10:32:19.246406    7792 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.3648016s)
I0515 10:32:19.249223    7792 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Jan\.minikube\machines\minikube\id_rsa...
I0515 10:32:23.731611    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:32:24.519881    7792 machine.go:94] provisionDockerMachine start ...
I0515 10:32:24.533867    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:25.594294    7792 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0604276s)
I0515 10:32:25.611290    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:25.612969    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:25.612969    7792 main.go:141] libmachine: About to run SSH command:
hostname
I0515 10:32:26.814507    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0515 10:32:26.826379    7792 ubuntu.go:169] provisioning hostname "minikube"
I0515 10:32:26.911984    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:27.832797    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:27.832797    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:27.832797    7792 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0515 10:32:28.347547    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0515 10:32:28.367441    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:29.206602    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:29.207770    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:29.207770    7792 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0515 10:32:29.599499    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0515 10:32:29.599499    7792 ubuntu.go:175] set auth options {CertDir:C:\Users\Jan\.minikube CaCertPath:C:\Users\Jan\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Jan\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Jan\.minikube\machines\server.pem ServerKeyPath:C:\Users\Jan\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Jan\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Jan\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Jan\.minikube}
I0515 10:32:29.601038    7792 ubuntu.go:177] setting up certificates
I0515 10:32:29.603189    7792 provision.go:84] configureAuth start
I0515 10:32:29.610600    7792 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 10:32:30.101487    7792 provision.go:143] copyHostCerts
I0515 10:32:30.103100    7792 exec_runner.go:151] cp: C:\Users\Jan\.minikube\certs\ca.pem --> C:\Users\Jan\.minikube/ca.pem (1070 bytes)
I0515 10:32:30.105393    7792 exec_runner.go:151] cp: C:\Users\Jan\.minikube\certs\cert.pem --> C:\Users\Jan\.minikube/cert.pem (1115 bytes)
I0515 10:32:30.106506    7792 exec_runner.go:151] cp: C:\Users\Jan\.minikube\certs\key.pem --> C:\Users\Jan\.minikube/key.pem (1675 bytes)
I0515 10:32:30.107596    7792 provision.go:117] generating server cert: C:\Users\Jan\.minikube\machines\server.pem ca-key=C:\Users\Jan\.minikube\certs\ca.pem private-key=C:\Users\Jan\.minikube\certs\ca-key.pem org=Jan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0515 10:32:30.533368    7792 provision.go:177] copyRemoteCerts
I0515 10:32:30.549105    7792 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0515 10:32:30.555580    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:31.145735    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:32:31.461612    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0515 10:32:31.616292    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0515 10:32:31.733559    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0515 10:32:31.841197    7792 provision.go:87] duration metric: took 2.2340817s to configureAuth
I0515 10:32:31.841197    7792 ubuntu.go:193] setting minikube options for container-runtime
I0515 10:32:31.849083    7792 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0515 10:32:31.866467    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:32.730626    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:32.730626    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:32.730626    7792 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0515 10:32:33.359001    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0515 10:32:33.359001    7792 ubuntu.go:71] root file system type: overlay
I0515 10:32:33.360027    7792 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0515 10:32:33.367231    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:33.689971    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:33.690495    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:33.690495    7792 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0515 10:32:33.959831    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0515 10:32:33.967574    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:34.373122    7792 main.go:141] libmachine: Using SSH client type: native
I0515 10:32:34.373122    7792 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x131a1c0] 0x131cda0 <nil>  [] 0s} 127.0.0.1 53412 <nil> <nil>}
I0515 10:32:34.373122    7792 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0515 10:32:37.457696    7792 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:59.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-15 02:32:33.931296135 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0515 10:32:37.457696    7792 machine.go:97] duration metric: took 12.9378149s to provisionDockerMachine
I0515 10:32:37.457696    7792 client.go:171] duration metric: took 1m9.8174457s to LocalClient.Create
I0515 10:32:37.457696    7792 start.go:167] duration metric: took 1m9.8174457s to libmachine.API.Create "minikube"
I0515 10:32:37.458218    7792 start.go:293] postStartSetup for "minikube" (driver="docker")
I0515 10:32:37.458737    7792 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0515 10:32:37.469769    7792 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0515 10:32:37.479128    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:37.837247    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:32:38.033942    7792 ssh_runner.go:195] Run: cat /etc/os-release
I0515 10:32:38.060066    7792 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0515 10:32:38.060066    7792 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0515 10:32:38.060066    7792 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0515 10:32:38.060066    7792 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0515 10:32:38.060606    7792 filesync.go:126] Scanning C:\Users\Jan\.minikube\addons for local assets ...
I0515 10:32:38.061684    7792 filesync.go:126] Scanning C:\Users\Jan\.minikube\files for local assets ...
I0515 10:32:38.062243    7792 start.go:296] duration metric: took 603.5065ms for postStartSetup
I0515 10:32:38.071326    7792 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 10:32:38.474033    7792 profile.go:143] Saving config to C:\Users\Jan\.minikube\profiles\minikube\config.json ...
I0515 10:32:38.488410    7792 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0515 10:32:38.496441    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:39.065035    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:32:39.309972    7792 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0515 10:32:39.329742    7792 start.go:128] duration metric: took 1m11.7301043s to createHost
I0515 10:32:39.329742    7792 start.go:83] releasing machines lock for "minikube", held for 1m11.7377643s
I0515 10:32:39.337289    7792 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0515 10:32:39.923429    7792 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0515 10:32:39.934280    7792 ssh_runner.go:195] Run: cat /version.json
I0515 10:32:39.936646    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:39.944181    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:32:40.476214    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:32:40.502648    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:32:41.311680    7792 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.3882508s)
I0515 10:32:41.311680    7792 ssh_runner.go:235] Completed: cat /version.json: (1.3773993s)
I0515 10:32:41.324999    7792 ssh_runner.go:195] Run: systemctl --version
I0515 10:32:41.358039    7792 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0515 10:32:41.447310    7792 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0515 10:32:41.541373    7792 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0515 10:32:41.559995    7792 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0515 10:32:41.705494    7792 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0515 10:32:41.705494    7792 start.go:494] detecting cgroup driver to use...
I0515 10:32:41.705494    7792 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0515 10:32:41.714440    7792 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0515 10:32:41.802446    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0515 10:32:41.870144    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0515 10:32:41.936109    7792 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0515 10:32:41.946467    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0515 10:32:42.017692    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0515 10:32:42.061055    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0515 10:32:42.115433    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0515 10:32:42.173987    7792 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0515 10:32:42.223509    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0515 10:32:42.266800    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0515 10:32:42.346588    7792 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0515 10:32:42.435684    7792 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0515 10:32:42.507832    7792 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0515 10:32:42.563745    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:32:42.926449    7792 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0515 10:32:43.267130    7792 start.go:494] detecting cgroup driver to use...
I0515 10:32:43.267667    7792 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0515 10:32:43.283246    7792 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0515 10:32:43.353373    7792 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0515 10:32:43.363410    7792 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0515 10:32:43.434584    7792 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0515 10:32:43.503441    7792 ssh_runner.go:195] Run: which cri-dockerd
I0515 10:32:43.539258    7792 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0515 10:32:43.617294    7792 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0515 10:32:43.745068    7792 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0515 10:32:44.105545    7792 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0515 10:32:44.363615    7792 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0515 10:32:44.372358    7792 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0515 10:32:44.427732    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:32:44.625276    7792 ssh_runner.go:195] Run: sudo systemctl restart docker
I0515 10:32:46.598885    7792 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.9736096s)
I0515 10:32:46.608578    7792 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0515 10:32:46.696410    7792 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0515 10:32:46.750842    7792 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0515 10:32:47.045625    7792 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0515 10:32:47.336061    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:32:47.636474    7792 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0515 10:32:47.720263    7792 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0515 10:32:47.775402    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:32:48.021642    7792 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0515 10:32:49.035192    7792 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (1.01355s)
I0515 10:32:49.035192    7792 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0515 10:32:49.046676    7792 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0515 10:32:49.083029    7792 start.go:562] Will wait 60s for crictl version
I0515 10:32:49.093729    7792 ssh_runner.go:195] Run: which crictl
I0515 10:32:49.139080    7792 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0515 10:32:49.461452    7792 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0515 10:32:49.468118    7792 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0515 10:32:49.849561    7792 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0515 10:32:49.944861    7792 out.go:204] ðŸ³  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0515 10:32:49.951924    7792 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0515 10:32:50.919849    7792 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0515 10:32:50.929823    7792 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0515 10:32:50.955294    7792 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0515 10:32:51.003541    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0515 10:32:51.630383    7792 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Jan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0515 10:32:51.632795    7792 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0515 10:32:51.648117    7792 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0515 10:32:51.749140    7792 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0515 10:32:51.749140    7792 docker.go:615] Images already preloaded, skipping extraction
I0515 10:32:51.757396    7792 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0515 10:32:51.820804    7792 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0515 10:32:51.822323    7792 cache_images.go:84] Images are preloaded, skipping loading
I0515 10:32:51.822323    7792 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0515 10:32:51.827835    7792 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0515 10:32:51.847186    7792 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0515 10:32:52.363867    7792 cni.go:84] Creating CNI manager for ""
I0515 10:32:52.363867    7792 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 10:32:52.364439    7792 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0515 10:32:52.365687    7792 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0515 10:32:52.367338    7792 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0515 10:32:52.379317    7792 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0515 10:32:52.447511    7792 binaries.go:44] Found k8s binaries, skipping transfer
I0515 10:32:52.475221    7792 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0515 10:32:52.515046    7792 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0515 10:32:52.578538    7792 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0515 10:32:52.644920    7792 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0515 10:32:52.702559    7792 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0515 10:32:52.725063    7792 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0515 10:32:52.782034    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:32:52.972329    7792 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0515 10:32:53.032242    7792 certs.go:68] Setting up C:\Users\Jan\.minikube\profiles\minikube for IP: 192.168.49.2
I0515 10:32:53.032242    7792 certs.go:194] generating shared ca certs ...
I0515 10:32:53.033198    7792 certs.go:226] acquiring lock for ca certs: {Name:mkdc8d0d960e099e616fb5cc262f74c41b4669d5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.033198    7792 certs.go:240] generating "minikubeCA" ca cert: C:\Users\Jan\.minikube\ca.key
I0515 10:32:53.407306    7792 crypto.go:156] Writing cert to C:\Users\Jan\.minikube\ca.crt ...
I0515 10:32:53.407306    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\ca.crt: {Name:mkcd614b6a4148cbfaa0c7916192c7a4f6ebf8b3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.408387    7792 crypto.go:164] Writing key to C:\Users\Jan\.minikube\ca.key ...
I0515 10:32:53.408387    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\ca.key: {Name:mkc93dfe5e2ac3de2ca3d6486bd97dbd4f986fee Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.408925    7792 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\Jan\.minikube\proxy-client-ca.key
I0515 10:32:53.507712    7792 crypto.go:156] Writing cert to C:\Users\Jan\.minikube\proxy-client-ca.crt ...
I0515 10:32:53.507712    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\proxy-client-ca.crt: {Name:mk4c69a0f379ffed41e758517e63d1fa5484b8fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.508811    7792 crypto.go:164] Writing key to C:\Users\Jan\.minikube\proxy-client-ca.key ...
I0515 10:32:53.508811    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\proxy-client-ca.key: {Name:mk21f2fdcd5c5ed1b16162b77b97d3c7034e6fec Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.509949    7792 certs.go:256] generating profile certs ...
I0515 10:32:53.511065    7792 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Jan\.minikube\profiles\minikube\client.key
I0515 10:32:53.511628    7792 crypto.go:68] Generating cert C:\Users\Jan\.minikube\profiles\minikube\client.crt with IP's: []
I0515 10:32:53.800732    7792 crypto.go:156] Writing cert to C:\Users\Jan\.minikube\profiles\minikube\client.crt ...
I0515 10:32:53.800732    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\client.crt: {Name:mk62fedf63ebd15b6ff97172e6276adba2b49976 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.801859    7792 crypto.go:164] Writing key to C:\Users\Jan\.minikube\profiles\minikube\client.key ...
I0515 10:32:53.801859    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\client.key: {Name:mk42acc4842569692f51a4f37cc72c427bb00ef9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.802367    7792 certs.go:363] generating signed profile cert for "minikube": C:\Users\Jan\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0515 10:32:53.802367    7792 crypto.go:68] Generating cert C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0515 10:32:53.958469    7792 crypto.go:156] Writing cert to C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0515 10:32:53.958469    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkfbeda0cde96a66c839aa95be789c42b1566702 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.959038    7792 crypto.go:164] Writing key to C:\Users\Jan\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0515 10:32:53.959038    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkbf09aee4a72507b56d490be66753783b815710 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:53.960093    7792 certs.go:381] copying C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt
I0515 10:32:53.976823    7792 certs.go:385] copying C:\Users\Jan\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Jan\.minikube\profiles\minikube\apiserver.key
I0515 10:32:54.000029    7792 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Jan\.minikube\profiles\minikube\proxy-client.key
I0515 10:32:54.000029    7792 crypto.go:68] Generating cert C:\Users\Jan\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0515 10:32:54.138667    7792 crypto.go:156] Writing cert to C:\Users\Jan\.minikube\profiles\minikube\proxy-client.crt ...
I0515 10:32:54.138667    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\proxy-client.crt: {Name:mk44e479bec8dba21fa7120aef454a1b794f4d85 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:54.139795    7792 crypto.go:164] Writing key to C:\Users\Jan\.minikube\profiles\minikube\proxy-client.key ...
I0515 10:32:54.139795    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.minikube\profiles\minikube\proxy-client.key: {Name:mkce2690bf083b837ace664f28ab52605c5c0f89 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:32:54.154447    7792 certs.go:484] found cert: C:\Users\Jan\.minikube\certs\ca-key.pem (1675 bytes)
I0515 10:32:54.154989    7792 certs.go:484] found cert: C:\Users\Jan\.minikube\certs\ca.pem (1070 bytes)
I0515 10:32:54.156039    7792 certs.go:484] found cert: C:\Users\Jan\.minikube\certs\cert.pem (1115 bytes)
I0515 10:32:54.156773    7792 certs.go:484] found cert: C:\Users\Jan\.minikube\certs\key.pem (1675 bytes)
I0515 10:32:54.171123    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0515 10:32:54.249107    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0515 10:32:54.331023    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0515 10:32:54.423019    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0515 10:32:54.515894    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0515 10:32:54.603390    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0515 10:32:54.669453    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0515 10:32:54.759742    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0515 10:32:54.849983    7792 ssh_runner.go:362] scp C:\Users\Jan\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0515 10:32:55.010895    7792 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0515 10:32:55.174659    7792 ssh_runner.go:195] Run: openssl version
I0515 10:32:55.246205    7792 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0515 10:32:55.306094    7792 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0515 10:32:55.378230    7792 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 15 02:32 /usr/share/ca-certificates/minikubeCA.pem
I0515 10:32:55.388324    7792 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0515 10:32:55.467335    7792 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0515 10:32:55.535758    7792 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0515 10:32:55.563162    7792 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0515 10:32:55.564232    7792 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Jan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0515 10:32:55.571252    7792 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0515 10:32:55.697395    7792 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0515 10:32:55.747027    7792 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0515 10:32:55.793504    7792 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0515 10:32:55.814317    7792 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0515 10:32:55.875568    7792 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0515 10:32:55.875568    7792 kubeadm.go:156] found existing configuration files:

I0515 10:32:55.892370    7792 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0515 10:32:55.949715    7792 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0515 10:32:55.959583    7792 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0515 10:32:56.027435    7792 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0515 10:32:56.090144    7792 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0515 10:32:56.103746    7792 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0515 10:32:56.158215    7792 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0515 10:32:56.212716    7792 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0515 10:32:56.244538    7792 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0515 10:32:56.292074    7792 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0515 10:32:56.332330    7792 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0515 10:32:56.345500    7792 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0515 10:32:56.399979    7792 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0515 10:32:57.259117    7792 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0515 10:32:57.512998    7792 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0515 10:33:22.743756    7792 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0515 10:33:22.743756    7792 kubeadm.go:309] [preflight] Running pre-flight checks
I0515 10:33:22.744262    7792 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0515 10:33:22.744262    7792 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0515 10:33:22.744262    7792 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0515 10:33:22.744786    7792 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0515 10:33:22.745310    7792 out.go:204]     â–ª Generating certificates and keys ...
I0515 10:33:22.745847    7792 kubeadm.go:309] [certs] Using existing ca certificate authority
I0515 10:33:22.745847    7792 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0515 10:33:22.746371    7792 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0515 10:33:22.746371    7792 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0515 10:33:22.746371    7792 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0515 10:33:22.746371    7792 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0515 10:33:22.746371    7792 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0515 10:33:22.746900    7792 kubeadm.go:309] [certs] Generating "sa" key and public key
I0515 10:33:22.747417    7792 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0515 10:33:22.747417    7792 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0515 10:33:22.747417    7792 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0515 10:33:22.747417    7792 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0515 10:33:22.747417    7792 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0515 10:33:22.747971    7792 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0515 10:33:22.747971    7792 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0515 10:33:22.747971    7792 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0515 10:33:22.749046    7792 out.go:204]     â–ª Booting up control plane ...
I0515 10:33:22.749046    7792 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0515 10:33:22.749046    7792 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0515 10:33:22.749578    7792 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0515 10:33:22.749578    7792 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0515 10:33:22.749578    7792 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0515 10:33:22.749578    7792 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0515 10:33:22.750102    7792 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0515 10:33:22.750102    7792 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0515 10:33:22.750102    7792 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.502140986s
I0515 10:33:22.750102    7792 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0515 10:33:22.750102    7792 kubeadm.go:309] [api-check] The API server is healthy after 13.502236239s
I0515 10:33:22.750634    7792 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0515 10:33:22.750634    7792 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0515 10:33:22.750634    7792 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0515 10:33:22.751155    7792 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0515 10:33:22.751155    7792 kubeadm.go:309] [bootstrap-token] Using token: t0rlys.s7x2va8l17wu7t18
I0515 10:33:22.752227    7792 out.go:204]     â–ª Configuring RBAC rules ...
I0515 10:33:22.752227    7792 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0515 10:33:22.752227    7792 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0515 10:33:22.752753    7792 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0515 10:33:22.752753    7792 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0515 10:33:22.752753    7792 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0515 10:33:22.753286    7792 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0515 10:33:22.753286    7792 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0515 10:33:22.753286    7792 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0515 10:33:22.753286    7792 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0515 10:33:22.753286    7792 kubeadm.go:309] 
I0515 10:33:22.753286    7792 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0515 10:33:22.753286    7792 kubeadm.go:309] 
I0515 10:33:22.753805    7792 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0515 10:33:22.753805    7792 kubeadm.go:309] 
I0515 10:33:22.753805    7792 kubeadm.go:309]   mkdir -p $HOME/.kube
I0515 10:33:22.753805    7792 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0515 10:33:22.753805    7792 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0515 10:33:22.753805    7792 kubeadm.go:309] 
I0515 10:33:22.753805    7792 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0515 10:33:22.754343    7792 kubeadm.go:309] 
I0515 10:33:22.754343    7792 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0515 10:33:22.754343    7792 kubeadm.go:309] 
I0515 10:33:22.755089    7792 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0515 10:33:22.755972    7792 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0515 10:33:22.755972    7792 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0515 10:33:22.755972    7792 kubeadm.go:309] 
I0515 10:33:22.755972    7792 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0515 10:33:22.756567    7792 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0515 10:33:22.756567    7792 kubeadm.go:309] 
I0515 10:33:22.756567    7792 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token t0rlys.s7x2va8l17wu7t18 \
I0515 10:33:22.757084    7792 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:02177ef28ae7264b8accb00dc5f70e2cdb6ae82c6876090e9d3c10131f9cc13a \
I0515 10:33:22.757084    7792 kubeadm.go:309] 	--control-plane 
I0515 10:33:22.759005    7792 kubeadm.go:309] 
I0515 10:33:22.759005    7792 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0515 10:33:22.759005    7792 kubeadm.go:309] 
I0515 10:33:22.759005    7792 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token t0rlys.s7x2va8l17wu7t18 \
I0515 10:33:22.759005    7792 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:02177ef28ae7264b8accb00dc5f70e2cdb6ae82c6876090e9d3c10131f9cc13a 
I0515 10:33:22.759005    7792 cni.go:84] Creating CNI manager for ""
I0515 10:33:22.759005    7792 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0515 10:33:22.761679    7792 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0515 10:33:22.773519    7792 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0515 10:33:22.788573    7792 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0515 10:33:22.821270    7792 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0515 10:33:22.836072    7792 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0515 10:33:22.836580    7792 ops.go:34] apiserver oom_adj: -16
I0515 10:33:22.838740    7792 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_15T10_33_22_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0515 10:33:23.799247    7792 kubeadm.go:1107] duration metric: took 975.2468ms to wait for elevateKubeSystemPrivileges
I0515 10:33:23.934487    7792 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_15T10_33_22_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.0956215s)
W0515 10:33:23.934487    7792 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0515 10:33:23.934487    7792 kubeadm.go:393] duration metric: took 28.3707847s to StartCluster
I0515 10:33:23.934992    7792 settings.go:142] acquiring lock: {Name:mkd1b866095e7b5bb4035f7db99c8a5725146199 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:33:23.935523    7792 settings.go:150] Updating kubeconfig:  C:\Users\Jan\.kube\config
I0515 10:33:23.942683    7792 lock.go:35] WriteFile acquiring C:\Users\Jan\.kube\config: {Name:mk984bfa0eff94bafcb0695b275fa424c056c974 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0515 10:33:23.944156    7792 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0515 10:33:23.945423    7792 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0515 10:33:23.944916    7792 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0515 10:33:23.945423    7792 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0515 10:33:23.946136    7792 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0515 10:33:23.947402    7792 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0515 10:33:23.947402    7792 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0515 10:33:23.947948    7792 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0515 10:33:23.948095    7792 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0515 10:33:23.948602    7792 host.go:66] Checking if "minikube" exists ...
I0515 10:33:23.959399    7792 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0515 10:33:24.009890    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:33:24.013373    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:33:24.307914    7792 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0515 10:33:24.330154    7792 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0515 10:33:24.477875    7792 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0515 10:33:24.480314    7792 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0515 10:33:24.480314    7792 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0515 10:33:24.489495    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:33:24.513343    7792 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0515 10:33:24.513343    7792 host.go:66] Checking if "minikube" exists ...
I0515 10:33:24.528673    7792 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0515 10:33:24.728688    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:33:24.745628    7792 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0515 10:33:24.745628    7792 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0515 10:33:24.753456    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0515 10:33:24.927222    7792 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53412 SSHKeyPath:C:\Users\Jan\.minikube\machines\minikube\id_rsa Username:docker}
I0515 10:33:24.932100    7792 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0515 10:33:24.948950    7792 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0515 10:33:24.957990    7792 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0515 10:33:25.286872    7792 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0515 10:33:25.311349    7792 api_server.go:52] waiting for apiserver process to appear ...
I0515 10:33:25.327671    7792 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0515 10:33:25.477190    7792 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0515 10:33:25.900146    7792 api_server.go:72] duration metric: took 1.9552305s to wait for apiserver process to appear ...
I0515 10:33:25.900146    7792 api_server.go:88] waiting for apiserver healthz status ...
I0515 10:33:25.900655    7792 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53409/healthz ...
I0515 10:33:25.912259    7792 api_server.go:279] https://127.0.0.1:53409/healthz returned 200:
ok
I0515 10:33:25.913382    7792 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0515 10:33:25.914452    7792 addons.go:505] duration metric: took 1.9695363s for enable addons: enabled=[storage-provisioner default-storageclass]
I0515 10:33:25.916391    7792 api_server.go:141] control plane version: v1.30.0
I0515 10:33:25.916391    7792 api_server.go:131] duration metric: took 16.2446ms to wait for apiserver health ...
I0515 10:33:25.917455    7792 system_pods.go:43] waiting for kube-system pods to appear ...
I0515 10:33:25.954016    7792 system_pods.go:59] 5 kube-system pods found
I0515 10:33:25.954016    7792 system_pods.go:61] "etcd-minikube" [fc8ed96c-4ee5-4c12-bbfb-a84a839dc94a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0515 10:33:25.954016    7792 system_pods.go:61] "kube-apiserver-minikube" [087dfc6e-0fda-40c6-91ed-c83dca01fbd2] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0515 10:33:25.954016    7792 system_pods.go:61] "kube-controller-manager-minikube" [f04a66c2-b255-4e3a-b603-384c50a0b2a4] Running
I0515 10:33:25.954016    7792 system_pods.go:61] "kube-scheduler-minikube" [d7351530-3373-40a9-a2a9-2504e3724dfc] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0515 10:33:25.954546    7792 system_pods.go:61] "storage-provisioner" [fb86b756-05a2-442d-8a4f-0f9d6c6a7a71] Pending
I0515 10:33:25.954546    7792 system_pods.go:74] duration metric: took 37.0911ms to wait for pod list to return data ...
I0515 10:33:25.954546    7792 kubeadm.go:576] duration metric: took 2.0096305s to wait for: map[apiserver:true system_pods:true]
I0515 10:33:25.954546    7792 node_conditions.go:102] verifying NodePressure condition ...
I0515 10:33:25.962033    7792 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0515 10:33:25.962033    7792 node_conditions.go:123] node cpu capacity is 4
I0515 10:33:25.963104    7792 node_conditions.go:105] duration metric: took 8.5574ms to run NodePressure ...
I0515 10:33:25.963104    7792 start.go:240] waiting for startup goroutines ...
I0515 10:33:25.963104    7792 start.go:245] waiting for cluster config update ...
I0515 10:33:25.963104    7792 start.go:254] writing updated cluster config ...
I0515 10:33:25.973950    7792 ssh_runner.go:195] Run: rm -f paused
I0515 10:33:27.056488    7792 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0515 10:33:27.064613    7792 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 15 02:32:44 minikube systemd[1]: Stopped Docker Application Container Engine.
May 15 02:32:44 minikube systemd[1]: Starting Docker Application Container Engine...
May 15 02:32:45 minikube dockerd[1191]: time="2024-05-15T02:32:45.066849263Z" level=info msg="Starting up"
May 15 02:32:45 minikube dockerd[1191]: time="2024-05-15T02:32:45.192433733Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 15 02:32:45 minikube dockerd[1191]: time="2024-05-15T02:32:45.283629196Z" level=info msg="Loading containers: start."
May 15 02:32:45 minikube dockerd[1191]: time="2024-05-15T02:32:45.860443752Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.153416215Z" level=info msg="Loading containers: done."
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311531775Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311563263Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311573222Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311579923Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311606435Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.311669602Z" level=info msg="Daemon has completed initialization"
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.566387658Z" level=info msg="API listen on /var/run/docker.sock"
May 15 02:32:46 minikube systemd[1]: Started Docker Application Container Engine.
May 15 02:32:46 minikube dockerd[1191]: time="2024-05-15T02:32:46.580037055Z" level=info msg="API listen on [::]:2376"
May 15 02:32:48 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Start docker client with request timeout 0s"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Loaded network plugin cni"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Docker cri networking managed by network plugin cni"
May 15 02:32:48 minikube cri-dockerd[1412]: time="2024-05-15T02:32:48Z" level=info msg="Setting cgroupDriver cgroupfs"
May 15 02:32:49 minikube cri-dockerd[1412]: time="2024-05-15T02:32:49Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 15 02:32:49 minikube cri-dockerd[1412]: time="2024-05-15T02:32:49Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 15 02:32:49 minikube cri-dockerd[1412]: time="2024-05-15T02:32:49Z" level=info msg="Start cri-dockerd grpc backend"
May 15 02:32:49 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 15 02:33:09 minikube cri-dockerd[1412]: time="2024-05-15T02:33:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/33018f05fc737549647b79363f6100d8bc623089f6d67f83ee2a51c91490f722/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:09 minikube cri-dockerd[1412]: time="2024-05-15T02:33:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5cd4aac33a617de9b547834cfd56c2e7c30197e819f2870e7e7b0adfce38a9c5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:09 minikube cri-dockerd[1412]: time="2024-05-15T02:33:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/033100ec8fee309144a7aef3367c600607194c03cf4c58f30fe369b0c6829aeb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:09 minikube cri-dockerd[1412]: time="2024-05-15T02:33:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7d58099ab9f6bd37387eae6d75d97f7d6158544f0e545c8fd00429cfc11ad07c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:37 minikube cri-dockerd[1412]: time="2024-05-15T02:33:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b35fae26d511838dc7d8e502f62d4565fead0e6547276a379a9cc0ce11008992/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:37 minikube cri-dockerd[1412]: time="2024-05-15T02:33:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a9226e076801db48d7c7a65b3b1e64e7a76d5af148d41b7e8b3a4c0127671e9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:37 minikube cri-dockerd[1412]: time="2024-05-15T02:33:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ee771c44cec6e82aa0890fd0218e173c694a1279fd171090ab7a3490f7017096/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 15 02:33:42 minikube cri-dockerd[1412]: time="2024-05-15T02:33:42Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 15 02:33:59 minikube dockerd[1191]: time="2024-05-15T02:33:59.444387024Z" level=info msg="ignoring event" container=5a0168b56bc330be813b2b0c9025f4bef7f430a2f4cd5f3f5aadde01e614f2e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 15 02:39:04 minikube cri-dockerd[1412]: time="2024-05-15T02:39:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea9e13ec8f47072d83081edb70ffe0bf52c83412e3e2ff4b348f78509f33c524/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 02:39:04 minikube cri-dockerd[1412]: time="2024-05-15T02:39:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93d14c83b1cc53324bad423adb6d43256a318733151bd5ccab730b27954580b2/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 02:39:05 minikube dockerd[1191]: time="2024-05-15T02:39:05.266337027Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" spanID=2e1b110e0cc4e234 traceID=65044691bb0e1f46a188006daec39b16
May 15 02:39:16 minikube cri-dockerd[1412]: time="2024-05-15T02:39:16Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [=====================>                             ]  32.32MB/75.78MB"
May 15 02:39:26 minikube cri-dockerd[1412]: time="2024-05-15T02:39:26Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [=====>                                             ]  8.356MB/75.78MB"
May 15 02:39:31 minikube cri-dockerd[1412]: time="2024-05-15T02:39:31Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
May 15 02:39:32 minikube dockerd[1191]: time="2024-05-15T02:39:32.514807516Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" spanID=0234c4e2a0341c85 traceID=758981444464a8d780c3b0877c02ed1b
May 15 02:39:39 minikube cri-dockerd[1412]: time="2024-05-15T02:39:39Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
May 15 05:59:08 minikube cri-dockerd[1412]: time="2024-05-15T05:59:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/24996ee8e36e20adb6e325625da800f2df5ccc08954844d4079593d4546d1355/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 05:59:22 minikube cri-dockerd[1412]: time="2024-05-15T05:59:22Z" level=info msg="Pulling image knuckiboy/k8net:latest: 89ef64df9fce: Downloading [===========>                                       ]  4.299MB/18.52MB"
May 15 05:59:32 minikube cri-dockerd[1412]: time="2024-05-15T05:59:32Z" level=info msg="Pulling image knuckiboy/k8net:latest: 89ef64df9fce: Downloading [======================>                            ]  8.223MB/18.52MB"
May 15 05:59:42 minikube cri-dockerd[1412]: time="2024-05-15T05:59:42Z" level=info msg="Pulling image knuckiboy/k8net:latest: 89ef64df9fce: Downloading [=========================================>         ]   15.5MB/18.52MB"
May 15 05:59:52 minikube cri-dockerd[1412]: time="2024-05-15T05:59:52Z" level=info msg="Pulling image knuckiboy/k8net:latest: bbf4a24316cb: Downloading [================>                                  ]  3.689MB/11.03MB"
May 15 06:00:02 minikube cri-dockerd[1412]: time="2024-05-15T06:00:02Z" level=info msg="Pulling image knuckiboy/k8net:latest: 09f376ebb190: Downloading [===========================================>       ]  25.47MB/29.15MB"
May 15 06:00:11 minikube cri-dockerd[1412]: time="2024-05-15T06:00:11Z" level=info msg="Stop pulling image knuckiboy/k8net:latest: Status: Downloaded newer image for knuckiboy/k8net:latest"
May 15 06:59:43 minikube cri-dockerd[1412]: time="2024-05-15T06:59:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09b748f89ecc2c91c9ea606221075329df9e40d21c24e4072a5699dccfc86caa/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 06:59:46 minikube cri-dockerd[1412]: time="2024-05-15T06:59:46Z" level=info msg="Stop pulling image knuckiboy/k8net:latest: Status: Image is up to date for knuckiboy/k8net:latest"
May 15 06:59:46 minikube dockerd[1191]: time="2024-05-15T06:59:46.914189491Z" level=info msg="ignoring event" container=198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 15 06:59:47 minikube dockerd[1191]: time="2024-05-15T06:59:47.201712616Z" level=info msg="ignoring event" container=24996ee8e36e20adb6e325625da800f2df5ccc08954844d4079593d4546d1355 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 15 07:05:54 minikube cri-dockerd[1412]: time="2024-05-15T07:05:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d359ed2a059898dbe6ce65d25507c4a786ff20255d040c7ce61c3858bb942cfc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 15 07:05:56 minikube cri-dockerd[1412]: time="2024-05-15T07:05:56Z" level=info msg="Stop pulling image knuckiboy/k8net:latest: Status: Image is up to date for knuckiboy/k8net:latest"
May 15 07:05:57 minikube dockerd[1191]: time="2024-05-15T07:05:57.312583446Z" level=info msg="ignoring event" container=8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 15 07:05:57 minikube dockerd[1191]: time="2024-05-15T07:05:57.523539539Z" level=info msg="ignoring event" container=09b748f89ecc2c91c9ea606221075329df9e40d21c24e4072a5699dccfc86caa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
3f1abdc5a019e       knuckiboy/k8net@sha256:2d0bd438d87cb6da418ad3957d3fd5fb53e4fe5e7b85365143fe8dbd615cfd70                17 minutes ago      Running             k8-net-web                  0                   d359ed2a05989       k8-net-web-674678d67d-j6rvn
56615abacafe0       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   5 hours ago         Running             dashboard-metrics-scraper   0                   93d14c83b1cc5       dashboard-metrics-scraper-b5fc48f67-v5jtm
b6d51eca253d7       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         5 hours ago         Running             kubernetes-dashboard        0                   ea9e13ec8f470       kubernetes-dashboard-779776cb65-z25q7
1cfdbde7ffa85       6e38f40d628db                                                                                          5 hours ago         Running             storage-provisioner         1                   ee771c44cec6e       storage-provisioner
5a0168b56bc33       6e38f40d628db                                                                                          5 hours ago         Exited              storage-provisioner         0                   ee771c44cec6e       storage-provisioner
623cd2e59eac7       cbb01a7bd410d                                                                                          5 hours ago         Running             coredns                     0                   2a9226e076801       coredns-7db6d8ff4d-xkf5c
519b358c04f89       a0bf559e280cf                                                                                          5 hours ago         Running             kube-proxy                  0                   b35fae26d5118       kube-proxy-js492
d831a418bb6df       c7aad43836fa5                                                                                          5 hours ago         Running             kube-controller-manager     0                   7d58099ab9f6b       kube-controller-manager-minikube
22917ee8becb9       3861cfcd7c04c                                                                                          5 hours ago         Running             etcd                        0                   033100ec8fee3       etcd-minikube
44f7895edb47d       c42f13656d0b2                                                                                          5 hours ago         Running             kube-apiserver              0                   33018f05fc737       kube-apiserver-minikube
9b703a2bcbc98       259c8277fcbbc                                                                                          5 hours ago         Running             kube-scheduler              0                   5cd4aac33a617       kube-scheduler-minikube


==> coredns [623cd2e59eac] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51707 - 28253 "HINFO IN 6519382129610732975.4626201334836047966. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.068100602s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[990915649]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (15-May-2024 02:33:38.174) (total time: 21072ms):
Trace[990915649]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21063ms (02:33:59.236)
Trace[990915649]: [21.072167422s] [21.072167422s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[254518259]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (15-May-2024 02:33:38.176) (total time: 21077ms):
Trace[254518259]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21075ms (02:33:59.250)
Trace[254518259]: [21.077342569s] [21.077342569s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1398300937]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (15-May-2024 02:33:38.161) (total time: 21093ms):
Trace[1398300937]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21091ms (02:33:59.251)
Trace[1398300937]: [21.093478588s] [21.093478588s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_15T10_33_22_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 15 May 2024 02:33:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 15 May 2024 07:23:10 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 15 May 2024 07:22:20 +0000   Wed, 15 May 2024 02:33:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 15 May 2024 07:22:20 +0000   Wed, 15 May 2024 02:33:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 15 May 2024 07:22:20 +0000   Wed, 15 May 2024 02:33:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 15 May 2024 07:22:20 +0000   Wed, 15 May 2024 02:33:22 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5942072Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5942072Ki
  pods:               110
System Info:
  Machine ID:                 313d959b474b4d568c2d31d62963ba6e
  System UUID:                313d959b474b4d568c2d31d62963ba6e
  Boot ID:                    6f762924-a4c4-406b-a972-58604d953f2a
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     k8-net-web-674678d67d-j6rvn                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 coredns-7db6d8ff4d-xkf5c                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     4h49m
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         4h49m
  kube-system                 kube-apiserver-minikube                      250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h49m
  kube-system                 kube-controller-manager-minikube             200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h49m
  kube-system                 kube-proxy-js492                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h49m
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h49m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h49m
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-v5jtm    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h44m
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-z25q7        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h44m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[May15 03:13] unchecked MSR access error: WRMSR to 0xda0 (tried to write 0x0000000000008000) at rIP: 0xffffffff8d0bfe97 (fpu__init_cpu_xstate+0x77/0xa0)
[  +0.000008] Call Trace:
[  +0.000003]  <TASK>
[  +0.000003]  ? ex_handler_msr.cold+0x2e/0x6a
[  +0.000004]  ? search_extable+0x22/0x30
[  +0.000004]  ? fixup_exception+0x242/0x2c0
[  +0.000003]  ? exc_general_protection+0xde/0x340
[  +0.000003]  ? console_unlock+0x2ec/0x4c0
[  +0.000004]  ? asm_exc_general_protection+0x22/0x30
[  +0.000003]  ? fpu__init_cpu_xstate+0x77/0xa0
[  +0.000002]  ? fpu__init_cpu_xstate+0x32/0xa0
[  +0.000001]  fpu__init_system_xstate+0xe9/0x8d3
[  +0.000007]  fpu__init_system+0xff/0x12e
[  +0.000002]  ? static_key_disable+0x16/0x20
[  +0.000003]  arch_cpu_finalize_init+0x1e/0x47
[  +0.000002]  start_kernel+0x5f3/0x69f
[  +0.000003]  secondary_startup_64_no_verify+0xb0/0xbb
[  +0.000004]  </TASK>
[  +0.099804] PCI: Fatal: No config space access function found
[  +0.030685] PCI: System does not support PCI
[  +0.738240] kvm: already loaded the other module
[  +8.040008] FS-Cache: Duplicate cookie detected
[  +0.691110] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.167741] FS-Cache: O-cookie d=00000000e59d2f13{9P.session} n=00000000e948d4e7
[  +0.314627] FS-Cache: O-key=[10] '34323934393338313836'
[  +0.142576] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.184471] FS-Cache: N-cookie d=00000000e59d2f13{9P.session} n=000000000f081a30
[  +0.084007] FS-Cache: N-key=[10] '34323934393338313836'
[  +4.002810] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.314198] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Singapore not found. Is the tzdata package installed?
[  +2.368417] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.021624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004246] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007569] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.815538] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.041417] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.093346] WSL (1) ERROR: ConfigMountFsTab:2579: Processing fstab with mount -a failed.
[  +0.021133] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.012798] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.034813] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.037950] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Singapore not found. Is the tzdata package installed?
[  +1.552372] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.171198] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.064727] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006641] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.264317] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [22917ee8becb] <==
{"level":"info","ts":"2024-05-15T05:51:25.289657Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1248129300,"revision":7439,"compact-revision":7199}
{"level":"info","ts":"2024-05-15T05:56:25.274947Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7680}
{"level":"info","ts":"2024-05-15T05:56:25.278413Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7680,"took":"3.218058ms","hash":1059009409,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1040384,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-15T05:56:25.278485Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1059009409,"revision":7680,"compact-revision":7439}
{"level":"info","ts":"2024-05-15T05:59:11.053438Z","caller":"etcdserver/server.go:1401","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-05-15T05:59:11.076017Z","caller":"etcdserver/server.go:2420","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2024-05-15T05:59:11.076393Z","caller":"etcdserver/server.go:2450","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2024-05-15T06:01:25.269755Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7920}
{"level":"info","ts":"2024-05-15T06:01:25.272534Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":7920,"took":"2.426594ms","hash":3433088331,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1089536,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:01:25.272587Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3433088331,"revision":7920,"compact-revision":7680}
{"level":"info","ts":"2024-05-15T06:06:25.26637Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8179}
{"level":"info","ts":"2024-05-15T06:06:25.283121Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8179,"took":"9.669711ms","hash":4156470464,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1101824,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:06:25.283172Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4156470464,"revision":8179,"compact-revision":7920}
{"level":"info","ts":"2024-05-15T06:11:25.262906Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8419}
{"level":"info","ts":"2024-05-15T06:11:25.267233Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8419,"took":"3.855298ms","hash":712558015,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1052672,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:11:25.267296Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":712558015,"revision":8419,"compact-revision":8179}
{"level":"info","ts":"2024-05-15T06:16:25.255777Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8660}
{"level":"info","ts":"2024-05-15T06:16:25.259366Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8660,"took":"3.323389ms","hash":2381832777,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1060864,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:16:25.259462Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2381832777,"revision":8660,"compact-revision":8419}
{"level":"info","ts":"2024-05-15T06:21:25.247366Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8899}
{"level":"info","ts":"2024-05-15T06:21:25.25079Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":8899,"took":"3.199268ms","hash":2595683190,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1060864,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:21:25.250848Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2595683190,"revision":8899,"compact-revision":8660}
{"level":"info","ts":"2024-05-15T06:26:25.243199Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9140}
{"level":"info","ts":"2024-05-15T06:26:25.247011Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9140,"took":"2.821065ms","hash":2268757192,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1056768,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:26:25.247075Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2268757192,"revision":9140,"compact-revision":8899}
{"level":"info","ts":"2024-05-15T06:31:25.236529Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9380}
{"level":"info","ts":"2024-05-15T06:31:25.241065Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9380,"took":"4.14319ms","hash":2220951269,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1060864,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:31:25.241157Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2220951269,"revision":9380,"compact-revision":9140}
{"level":"info","ts":"2024-05-15T06:36:25.23152Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9620}
{"level":"info","ts":"2024-05-15T06:36:25.234552Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9620,"took":"2.800949ms","hash":263868938,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1052672,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T06:36:25.234613Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":263868938,"revision":9620,"compact-revision":9380}
{"level":"info","ts":"2024-05-15T06:41:25.228813Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9860}
{"level":"info","ts":"2024-05-15T06:41:25.233585Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":9860,"took":"4.531695ms","hash":1106227888,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1036288,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-15T06:41:25.233666Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1106227888,"revision":9860,"compact-revision":9620}
{"level":"info","ts":"2024-05-15T06:46:25.225357Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10099}
{"level":"info","ts":"2024-05-15T06:46:25.228136Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10099,"took":"2.493617ms","hash":1551836437,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1048576,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-05-15T06:46:25.228192Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1551836437,"revision":10099,"compact-revision":9860}
{"level":"warn","ts":"2024-05-15T06:49:24.294472Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"181.103738ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-15T06:49:24.313267Z","caller":"traceutil/trace.go:171","msg":"trace[789346090] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10483; }","duration":"214.706771ms","start":"2024-05-15T06:49:24.079979Z","end":"2024-05-15T06:49:24.294686Z","steps":["trace[789346090] 'range keys from in-memory index tree'  (duration: 181.004595ms)"],"step_count":1}
{"level":"info","ts":"2024-05-15T06:51:25.229568Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10339}
{"level":"info","ts":"2024-05-15T06:51:25.237893Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10339,"took":"8.059612ms","hash":2341472219,"current-db-size-bytes":2400256,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1294336,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-05-15T06:51:25.237949Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2341472219,"revision":10339,"compact-revision":10099}
{"level":"info","ts":"2024-05-15T06:56:25.22677Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10581}
{"level":"info","ts":"2024-05-15T06:56:25.233776Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10581,"took":"5.608733ms","hash":785921873,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1564672,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-05-15T06:56:25.233845Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":785921873,"revision":10581,"compact-revision":10339}
{"level":"info","ts":"2024-05-15T07:01:25.222064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10827}
{"level":"info","ts":"2024-05-15T07:01:25.230381Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10827,"took":"7.349527ms","hash":3458759421,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-15T07:01:25.230446Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3458759421,"revision":10827,"compact-revision":10581}
{"level":"info","ts":"2024-05-15T07:06:25.214967Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11102}
{"level":"info","ts":"2024-05-15T07:06:25.219384Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11102,"took":"3.876098ms","hash":809000814,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1409024,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-15T07:06:25.219455Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":809000814,"revision":11102,"compact-revision":10827}
{"level":"info","ts":"2024-05-15T07:11:25.212005Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11375}
{"level":"info","ts":"2024-05-15T07:11:25.218735Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11375,"took":"6.500791ms","hash":1059925679,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1204224,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-05-15T07:11:25.218796Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1059925679,"revision":11375,"compact-revision":11102}
{"level":"info","ts":"2024-05-15T07:16:25.209948Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11614}
{"level":"info","ts":"2024-05-15T07:16:25.213338Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11614,"took":"2.926391ms","hash":3114418282,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1101824,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T07:16:25.213413Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3114418282,"revision":11614,"compact-revision":11375}
{"level":"info","ts":"2024-05-15T07:21:25.204989Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11855}
{"level":"info","ts":"2024-05-15T07:21:25.208815Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11855,"took":"3.574984ms","hash":2375636101,"current-db-size-bytes":2527232,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1110016,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-05-15T07:21:25.208906Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2375636101,"revision":11855,"compact-revision":11614}


==> kernel <==
 07:23:18 up  4:10,  0 users,  load average: 0.65, 0.32, 0.25
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [44f7895edb47] <==
I0515 02:33:17.555321       1 crd_finalizer.go:266] Starting CRDFinalizer
I0515 02:33:17.555355       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0515 02:33:17.556466       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0515 02:33:17.556694       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0515 02:33:17.556962       1 controller.go:78] Starting OpenAPI AggregationController
I0515 02:33:17.557152       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0515 02:33:17.558041       1 available_controller.go:423] Starting AvailableConditionController
I0515 02:33:17.567760       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0515 02:33:17.573245       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0515 02:33:17.575659       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0515 02:33:17.577906       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0515 02:33:17.577965       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0515 02:33:17.563538       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0515 02:33:17.563656       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0515 02:33:17.584795       1 controller.go:116] Starting legacy_token_tracking_controller
I0515 02:33:17.584827       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0515 02:33:17.662660       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0515 02:33:17.679259       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0515 02:33:17.680588       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0515 02:33:17.681487       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0515 02:33:17.682140       1 aggregator.go:165] initial CRD sync complete...
I0515 02:33:17.682156       1 autoregister_controller.go:141] Starting autoregister controller
I0515 02:33:17.682162       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0515 02:33:17.682168       1 cache.go:39] Caches are synced for autoregister controller
I0515 02:33:17.686619       1 shared_informer.go:320] Caches are synced for configmaps
I0515 02:33:17.731062       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0515 02:33:17.731093       1 policy_source.go:224] refreshing policies
http2: server: error reading preface from client 192.168.49.2:54546: read tcp 192.168.49.2:8443->192.168.49.2:54546: read: connection reset by peer
I0515 02:33:17.756258       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0515 02:33:17.756380       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0515 02:33:17.763854       1 controller.go:615] quota admission added evaluator for: namespaces
I0515 02:33:17.769754       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0515 02:33:17.839199       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0515 02:33:17.861110       1 shared_informer.go:320] Caches are synced for node_authorizer
I0515 02:33:18.603199       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0515 02:33:18.680035       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0515 02:33:18.680059       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0515 02:33:20.858286       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0515 02:33:20.933758       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0515 02:33:21.084063       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0515 02:33:21.096911       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0515 02:33:21.098500       1 controller.go:615] quota admission added evaluator for: endpoints
I0515 02:33:21.106414       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0515 02:33:21.623922       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0515 02:33:22.166323       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0515 02:33:22.291341       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0515 02:33:22.304685       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0515 02:33:35.476685       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0515 02:33:36.384478       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0515 02:39:03.556175       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.110.144.38"}
I0515 02:39:03.588851       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.99.217.48"}
E0515 04:22:57.746098       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0515 04:22:57.973290       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0515 04:57:46.728887       1 trace.go:236] Trace[1189796198]: "Update" accept:application/json, */*,audit-id:65e9a8c7-cc16-4677-b21a-e160ccdad9a3,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (15-May-2024 04:57:45.924) (total time: 720ms):
Trace[1189796198]: ["GuaranteedUpdate etcd3" audit-id:65e9a8c7-cc16-4677-b21a-e160ccdad9a3,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 720ms (04:57:45.925)
Trace[1189796198]:  ---"Txn call completed" 719ms (04:57:46.644)]
Trace[1189796198]: [720.249611ms] [720.249611ms] END
E0515 05:24:46.250658       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0515 05:24:46.418129       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0515 06:51:33.976861       1 alloc.go:330] "allocated clusterIPs" service="default/k8net-web-service" clusterIPs={"IPv4":"10.107.16.34"}


==> kube-controller-manager [d831a418bb6d] <==
I0515 02:39:03.348359       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="30.946198ms"
E0515 02:39:03.348470       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0515 02:39:03.352626       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="20.699819ms"
E0515 02:39:03.352694       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0515 02:39:03.355674       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="6.003275ms"
E0515 02:39:03.355797       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0515 02:39:03.359795       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="7.064121ms"
E0515 02:39:03.360069       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" failed with pods "dashboard-metrics-scraper-b5fc48f67-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0515 02:39:03.365630       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="9.583435ms"
E0515 02:39:03.365773       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-779776cb65" failed with pods "kubernetes-dashboard-779776cb65-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0515 02:39:03.397114       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="31.234778ms"
I0515 02:39:03.399857       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="26.686204ms"
I0515 02:39:03.415463       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="18.278278ms"
I0515 02:39:03.415463       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="15.301764ms"
I0515 02:39:03.415543       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="47.074Âµs"
I0515 02:39:03.480442       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="64.923013ms"
I0515 02:39:03.480900       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="161.019Âµs"
I0515 02:39:03.495077       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="47.273Âµs"
I0515 02:39:32.826240       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="13.633031ms"
I0515 02:39:32.827367       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="43.704Âµs"
I0515 02:39:39.891679       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="16.610563ms"
I0515 02:39:39.891852       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="100.417Âµs"
E0515 04:22:57.753870       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0515 04:22:57.977073       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0515 04:49:23.028818       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-6rszl" approvedExpiration="1h0m0s"
E0515 05:24:46.251560       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0515 05:24:46.418716       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0515 05:59:07.298856       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="75.687814ms"
I0515 05:59:07.323710       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="24.482308ms"
I0515 05:59:07.326586       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="105.476Âµs"
I0515 05:59:07.347396       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="82.483Âµs"
I0515 05:59:07.412352       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="40.553Âµs"
I0515 06:00:12.462043       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="9.408192ms"
I0515 06:00:12.468363       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="6.243684ms"
I0515 06:59:42.355433       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="41.679946ms"
I0515 06:59:42.383443       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="27.709999ms"
I0515 06:59:42.383509       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="32.507Âµs"
I0515 06:59:42.395475       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="39.348Âµs"
I0515 06:59:46.586858       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="8.436971ms"
I0515 06:59:46.586965       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="30.642Âµs"
I0515 06:59:46.621708       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="25.791592ms"
I0515 06:59:46.629733       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="7.940705ms"
I0515 06:59:46.630498       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="63.048Âµs"
I0515 06:59:47.331506       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="42.193Âµs"
I0515 06:59:47.597912       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="40.716Âµs"
I0515 06:59:47.623100       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="420.536Âµs"
I0515 06:59:47.643095       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-685f67cd5c" duration="99.007Âµs"
I0515 07:05:53.276574       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="27.219608ms"
I0515 07:05:53.301561       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="24.801445ms"
I0515 07:05:53.301638       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="45.216Âµs"
I0515 07:05:53.311057       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="42.295Âµs"
I0515 07:05:57.139187       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="9.059942ms"
I0515 07:05:57.139429       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-674678d67d" duration="176.184Âµs"
I0515 07:05:57.169336       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="25.172136ms"
I0515 07:05:57.205035       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="35.646653ms"
I0515 07:05:57.205435       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="44.428Âµs"
I0515 07:05:57.600009       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="51.109Âµs"
I0515 07:05:58.158940       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="45.878Âµs"
I0515 07:05:58.174167       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="38.236Âµs"
I0515 07:05:58.179080       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/k8-net-web-7db76c48c6" duration="73.33Âµs"


==> kube-proxy [519b358c04f8] <==
I0515 02:33:37.950250       1 server_linux.go:69] "Using iptables proxy"
I0515 02:33:38.045282       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0515 02:33:38.168095       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0515 02:33:38.169122       1 server_linux.go:165] "Using iptables Proxier"
I0515 02:33:38.174112       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0515 02:33:38.174142       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0515 02:33:38.174759       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0515 02:33:38.179553       1 server.go:872] "Version info" version="v1.30.0"
I0515 02:33:38.179902       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0515 02:33:38.187006       1 config.go:192] "Starting service config controller"
I0515 02:33:38.187131       1 shared_informer.go:313] Waiting for caches to sync for service config
I0515 02:33:38.187395       1 config.go:101] "Starting endpoint slice config controller"
I0515 02:33:38.187408       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0515 02:33:38.187395       1 config.go:319] "Starting node config controller"
I0515 02:33:38.196442       1 shared_informer.go:313] Waiting for caches to sync for node config
I0515 02:33:38.288323       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0515 02:33:38.288323       1 shared_informer.go:320] Caches are synced for service config
I0515 02:33:38.297513       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [9b703a2bcbc9] <==
E0515 02:33:17.865725       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0515 02:33:17.873005       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0515 02:33:17.874463       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:17.885693       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:17.884203       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0515 02:33:17.885835       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0515 02:33:17.884335       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:17.885983       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:17.884448       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:17.886107       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:17.884520       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0515 02:33:17.886240       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0515 02:33:17.884583       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:17.886371       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:17.884646       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0515 02:33:17.886505       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0515 02:33:17.884701       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0515 02:33:17.886641       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0515 02:33:17.884866       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0515 02:33:17.886775       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0515 02:33:17.885006       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0515 02:33:17.886909       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0515 02:33:17.885311       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0515 02:33:17.887048       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0515 02:33:17.887138       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0515 02:33:17.897585       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0515 02:33:17.897697       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0515 02:33:18.788808       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0515 02:33:18.788851       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0515 02:33:18.816304       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0515 02:33:18.816399       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0515 02:33:18.907382       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0515 02:33:18.907644       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0515 02:33:18.917813       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0515 02:33:18.934805       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0515 02:33:19.060535       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0515 02:33:19.060609       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0515 02:33:19.199827       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0515 02:33:19.200128       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0515 02:33:19.235711       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:19.235771       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:19.235885       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0515 02:33:19.235899       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0515 02:33:19.308832       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:19.308884       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:19.334547       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0515 02:33:19.334650       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0515 02:33:19.350699       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:19.350778       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:19.350915       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0515 02:33:19.350935       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0515 02:33:19.380500       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0515 02:33:19.385796       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0515 02:33:19.404373       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0515 02:33:19.404458       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0515 02:33:19.433086       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0515 02:33:19.433144       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0515 02:33:20.465495       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0515 02:33:20.471130       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
I0515 02:33:21.721787       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 15 02:33:35 minikube kubelet[2336]: I0515 02:33:35.677880    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ca2b2d9e-1d99-4794-89d2-2727285ccc4c-lib-modules\") pod \"kube-proxy-js492\" (UID: \"ca2b2d9e-1d99-4794-89d2-2727285ccc4c\") " pod="kube-system/kube-proxy-js492"
May 15 02:33:35 minikube kubelet[2336]: I0515 02:33:35.677906    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hjpk8\" (UniqueName: \"kubernetes.io/projected/ca2b2d9e-1d99-4794-89d2-2727285ccc4c-kube-api-access-hjpk8\") pod \"kube-proxy-js492\" (UID: \"ca2b2d9e-1d99-4794-89d2-2727285ccc4c\") " pod="kube-system/kube-proxy-js492"
May 15 02:33:35 minikube kubelet[2336]: I0515 02:33:35.677964    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ca2b2d9e-1d99-4794-89d2-2727285ccc4c-kube-proxy\") pod \"kube-proxy-js492\" (UID: \"ca2b2d9e-1d99-4794-89d2-2727285ccc4c\") " pod="kube-system/kube-proxy-js492"
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.685237    2336 projected.go:294] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.685718    2336 projected.go:200] Error preparing data for projected volume kube-api-access-2h9sn for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.685935    2336 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fb86b756-05a2-442d-8a4f-0f9d6c6a7a71-kube-api-access-2h9sn podName:fb86b756-05a2-442d-8a4f-0f9d6c6a7a71 nodeName:}" failed. No retries permitted until 2024-05-15 02:33:36.185898105 +0000 UTC m=+14.140813555 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-2h9sn" (UniqueName: "kubernetes.io/projected/fb86b756-05a2-442d-8a4f-0f9d6c6a7a71-kube-api-access-2h9sn") pod "storage-provisioner" (UID: "fb86b756-05a2-442d-8a4f-0f9d6c6a7a71") : configmap "kube-root-ca.crt" not found
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.788202    2336 projected.go:294] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.788239    2336 projected.go:200] Error preparing data for projected volume kube-api-access-hjpk8 for pod kube-system/kube-proxy-js492: configmap "kube-root-ca.crt" not found
May 15 02:33:35 minikube kubelet[2336]: E0515 02:33:35.788295    2336 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ca2b2d9e-1d99-4794-89d2-2727285ccc4c-kube-api-access-hjpk8 podName:ca2b2d9e-1d99-4794-89d2-2727285ccc4c nodeName:}" failed. No retries permitted until 2024-05-15 02:33:36.288273491 +0000 UTC m=+14.243188941 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-hjpk8" (UniqueName: "kubernetes.io/projected/ca2b2d9e-1d99-4794-89d2-2727285ccc4c-kube-api-access-hjpk8") pod "kube-proxy-js492" (UID: "ca2b2d9e-1d99-4794-89d2-2727285ccc4c") : configmap "kube-root-ca.crt" not found
May 15 02:33:36 minikube kubelet[2336]: E0515 02:33:36.283645    2336 projected.go:294] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
May 15 02:33:36 minikube kubelet[2336]: E0515 02:33:36.283716    2336 projected.go:200] Error preparing data for projected volume kube-api-access-2h9sn for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
May 15 02:33:36 minikube kubelet[2336]: E0515 02:33:36.283789    2336 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fb86b756-05a2-442d-8a4f-0f9d6c6a7a71-kube-api-access-2h9sn podName:fb86b756-05a2-442d-8a4f-0f9d6c6a7a71 nodeName:}" failed. No retries permitted until 2024-05-15 02:33:37.283762977 +0000 UTC m=+15.238678429 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-2h9sn" (UniqueName: "kubernetes.io/projected/fb86b756-05a2-442d-8a4f-0f9d6c6a7a71-kube-api-access-2h9sn") pod "storage-provisioner" (UID: "fb86b756-05a2-442d-8a4f-0f9d6c6a7a71") : configmap "kube-root-ca.crt" not found
May 15 02:33:36 minikube kubelet[2336]: I0515 02:33:36.678244    2336 topology_manager.go:215] "Topology Admit Handler" podUID="14e1066c-4cb6-418c-82fb-64e1ea146b1e" podNamespace="kube-system" podName="coredns-7db6d8ff4d-xkf5c"
May 15 02:33:36 minikube kubelet[2336]: I0515 02:33:36.789749    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/14e1066c-4cb6-418c-82fb-64e1ea146b1e-config-volume\") pod \"coredns-7db6d8ff4d-xkf5c\" (UID: \"14e1066c-4cb6-418c-82fb-64e1ea146b1e\") " pod="kube-system/coredns-7db6d8ff4d-xkf5c"
May 15 02:33:36 minikube kubelet[2336]: I0515 02:33:36.789876    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jfwkp\" (UniqueName: \"kubernetes.io/projected/14e1066c-4cb6-418c-82fb-64e1ea146b1e-kube-api-access-jfwkp\") pod \"coredns-7db6d8ff4d-xkf5c\" (UID: \"14e1066c-4cb6-418c-82fb-64e1ea146b1e\") " pod="kube-system/coredns-7db6d8ff4d-xkf5c"
May 15 02:33:37 minikube kubelet[2336]: I0515 02:33:37.952757    2336 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ee771c44cec6e82aa0890fd0218e173c694a1279fd171090ab7a3490f7017096"
May 15 02:33:38 minikube kubelet[2336]: I0515 02:33:38.102124    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-xkf5c" podStartSLOduration=2.102099034 podStartE2EDuration="2.102099034s" podCreationTimestamp="2024-05-15 02:33:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-15 02:33:38.042740865 +0000 UTC m=+15.997656319" watchObservedRunningTime="2024-05-15 02:33:38.102099034 +0000 UTC m=+16.057014489"
May 15 02:33:39 minikube kubelet[2336]: I0515 02:33:39.055612    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=14.05558721 podStartE2EDuration="14.05558721s" podCreationTimestamp="2024-05-15 02:33:25 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-15 02:33:39.055382822 +0000 UTC m=+17.010298298" watchObservedRunningTime="2024-05-15 02:33:39.05558721 +0000 UTC m=+17.010502662"
May 15 02:33:39 minikube kubelet[2336]: I0515 02:33:39.055794    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-js492" podStartSLOduration=4.055730823 podStartE2EDuration="4.055730823s" podCreationTimestamp="2024-05-15 02:33:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-05-15 02:33:38.102772119 +0000 UTC m=+16.057687573" watchObservedRunningTime="2024-05-15 02:33:39.055730823 +0000 UTC m=+17.010646293"
May 15 02:33:42 minikube kubelet[2336]: I0515 02:33:42.806814    2336 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
May 15 02:33:42 minikube kubelet[2336]: I0515 02:33:42.807883    2336 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
May 15 02:34:00 minikube kubelet[2336]: I0515 02:34:00.285086    2336 scope.go:117] "RemoveContainer" containerID="5a0168b56bc330be813b2b0c9025f4bef7f430a2f4cd5f3f5aadde01e614f2e2"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.407301    2336 topology_manager.go:215] "Topology Admit Handler" podUID="9808fdb2-4fd9-42d4-9219-00f9d719e3f2" podNamespace="kubernetes-dashboard" podName="dashboard-metrics-scraper-b5fc48f67-v5jtm"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.418806    2336 topology_manager.go:215] "Topology Admit Handler" podUID="bcd657f7-cbbe-4059-802f-c8803c06e989" podNamespace="kubernetes-dashboard" podName="kubernetes-dashboard-779776cb65-z25q7"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.479443    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-grhmv\" (UniqueName: \"kubernetes.io/projected/bcd657f7-cbbe-4059-802f-c8803c06e989-kube-api-access-grhmv\") pod \"kubernetes-dashboard-779776cb65-z25q7\" (UID: \"bcd657f7-cbbe-4059-802f-c8803c06e989\") " pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-z25q7"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.479562    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xvc6v\" (UniqueName: \"kubernetes.io/projected/9808fdb2-4fd9-42d4-9219-00f9d719e3f2-kube-api-access-xvc6v\") pod \"dashboard-metrics-scraper-b5fc48f67-v5jtm\" (UID: \"9808fdb2-4fd9-42d4-9219-00f9d719e3f2\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-v5jtm"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.479589    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/9808fdb2-4fd9-42d4-9219-00f9d719e3f2-tmp-volume\") pod \"dashboard-metrics-scraper-b5fc48f67-v5jtm\" (UID: \"9808fdb2-4fd9-42d4-9219-00f9d719e3f2\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-v5jtm"
May 15 02:39:03 minikube kubelet[2336]: I0515 02:39:03.479611    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/bcd657f7-cbbe-4059-802f-c8803c06e989-tmp-volume\") pod \"kubernetes-dashboard-779776cb65-z25q7\" (UID: \"bcd657f7-cbbe-4059-802f-c8803c06e989\") " pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-z25q7"
May 15 02:39:32 minikube kubelet[2336]: I0515 02:39:32.812557    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-z25q7" podStartSLOduration=2.488807424 podStartE2EDuration="29.812346548s" podCreationTimestamp="2024-05-15 02:39:03 +0000 UTC" firstStartedPulling="2024-05-15 02:39:04.366299158 +0000 UTC m=+342.344289786" lastFinishedPulling="2024-05-15 02:39:31.688466759 +0000 UTC m=+369.667828910" observedRunningTime="2024-05-15 02:39:32.812339998 +0000 UTC m=+370.791702133" watchObservedRunningTime="2024-05-15 02:39:32.812346548 +0000 UTC m=+370.791708681"
May 15 05:59:07 minikube kubelet[2336]: I0515 05:59:07.357692    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-v5jtm" podStartSLOduration=11969.657632898 podStartE2EDuration="3h20m4.354779827s" podCreationTimestamp="2024-05-15 02:39:03 +0000 UTC" firstStartedPulling="2024-05-15 02:39:04.366301615 +0000 UTC m=+342.344292245" lastFinishedPulling="2024-05-15 02:39:39.062077042 +0000 UTC m=+377.041439174" observedRunningTime="2024-05-15 02:39:39.877390696 +0000 UTC m=+377.856752832" watchObservedRunningTime="2024-05-15 05:59:07.354779827 +0000 UTC m=+9453.798404567"
May 15 05:59:07 minikube kubelet[2336]: I0515 05:59:07.371242    2336 topology_manager.go:215] "Topology Admit Handler" podUID="1d8e2150-91cf-4701-8b6d-f8fac427aa86" podNamespace="default" podName="k8-net-web-685f67cd5c-g6g8b"
May 15 05:59:07 minikube kubelet[2336]: I0515 05:59:07.553436    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nnbbz\" (UniqueName: \"kubernetes.io/projected/1d8e2150-91cf-4701-8b6d-f8fac427aa86-kube-api-access-nnbbz\") pod \"k8-net-web-685f67cd5c-g6g8b\" (UID: \"1d8e2150-91cf-4701-8b6d-f8fac427aa86\") " pod="default/k8-net-web-685f67cd5c-g6g8b"
May 15 05:59:08 minikube kubelet[2336]: I0515 05:59:08.883052    2336 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="24996ee8e36e20adb6e325625da800f2df5ccc08954844d4079593d4546d1355"
May 15 06:00:12 minikube kubelet[2336]: I0515 06:00:12.457568    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/k8-net-web-685f67cd5c-g6g8b" podStartSLOduration=3.01385077 podStartE2EDuration="1m5.457546013s" podCreationTimestamp="2024-05-15 05:59:07 +0000 UTC" firstStartedPulling="2024-05-15 05:59:09.080421208 +0000 UTC m=+9455.524045926" lastFinishedPulling="2024-05-15 06:00:11.522007999 +0000 UTC m=+9517.967741169" observedRunningTime="2024-05-15 06:00:12.457204949 +0000 UTC m=+9518.902938137" watchObservedRunningTime="2024-05-15 06:00:12.457546013 +0000 UTC m=+9518.903279175"
May 15 06:59:42 minikube kubelet[2336]: I0515 06:59:42.365311    2336 topology_manager.go:215] "Topology Admit Handler" podUID="71442dd9-1bc0-497f-92b2-b6084b681ef3" podNamespace="default" podName="k8-net-web-7db76c48c6-6bxcf"
May 15 06:59:42 minikube kubelet[2336]: I0515 06:59:42.485443    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rxkhd\" (UniqueName: \"kubernetes.io/projected/71442dd9-1bc0-497f-92b2-b6084b681ef3-kube-api-access-rxkhd\") pod \"k8-net-web-7db76c48c6-6bxcf\" (UID: \"71442dd9-1bc0-497f-92b2-b6084b681ef3\") " pod="default/k8-net-web-7db76c48c6-6bxcf"
May 15 06:59:43 minikube kubelet[2336]: I0515 06:59:43.514492    2336 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="09b748f89ecc2c91c9ea606221075329df9e40d21c24e4072a5699dccfc86caa"
May 15 06:59:46 minikube kubelet[2336]: I0515 06:59:46.611862    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/k8-net-web-7db76c48c6-6bxcf" podStartSLOduration=2.103252357 podStartE2EDuration="4.611595765s" podCreationTimestamp="2024-05-15 06:59:42 +0000 UTC" firstStartedPulling="2024-05-15 06:59:43.655728291 +0000 UTC m=+13090.261698080" lastFinishedPulling="2024-05-15 06:59:46.164071699 +0000 UTC m=+13092.770041488" observedRunningTime="2024-05-15 06:59:46.579853381 +0000 UTC m=+13093.185823175" watchObservedRunningTime="2024-05-15 06:59:46.611595765 +0000 UTC m=+13093.217565556"
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.334838    2336 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-nnbbz\" (UniqueName: \"kubernetes.io/projected/1d8e2150-91cf-4701-8b6d-f8fac427aa86-kube-api-access-nnbbz\") pod \"1d8e2150-91cf-4701-8b6d-f8fac427aa86\" (UID: \"1d8e2150-91cf-4701-8b6d-f8fac427aa86\") "
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.345134    2336 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1d8e2150-91cf-4701-8b6d-f8fac427aa86-kube-api-access-nnbbz" (OuterVolumeSpecName: "kube-api-access-nnbbz") pod "1d8e2150-91cf-4701-8b6d-f8fac427aa86" (UID: "1d8e2150-91cf-4701-8b6d-f8fac427aa86"). InnerVolumeSpecName "kube-api-access-nnbbz". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.435706    2336 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-nnbbz\" (UniqueName: \"kubernetes.io/projected/1d8e2150-91cf-4701-8b6d-f8fac427aa86-kube-api-access-nnbbz\") on node \"minikube\" DevicePath \"\""
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.581411    2336 scope.go:117] "RemoveContainer" containerID="198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5"
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.632907    2336 scope.go:117] "RemoveContainer" containerID="198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5"
May 15 06:59:47 minikube kubelet[2336]: E0515 06:59:47.642233    2336 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5" containerID="198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5"
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.642797    2336 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5"} err="failed to get container status \"198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5\": rpc error: code = Unknown desc = Error response from daemon: No such container: 198b2e294db244258c0139b6c17a92dc41af52953be549979bfb46dc3d579be5"
May 15 06:59:47 minikube kubelet[2336]: I0515 06:59:47.840394    2336 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="1d8e2150-91cf-4701-8b6d-f8fac427aa86" path="/var/lib/kubelet/pods/1d8e2150-91cf-4701-8b6d-f8fac427aa86/volumes"
May 15 07:05:53 minikube kubelet[2336]: I0515 07:05:53.287825    2336 topology_manager.go:215] "Topology Admit Handler" podUID="c783c68e-13c3-4924-a633-264c9ca9f607" podNamespace="default" podName="k8-net-web-674678d67d-j6rvn"
May 15 07:05:53 minikube kubelet[2336]: E0515 07:05:53.290590    2336 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1d8e2150-91cf-4701-8b6d-f8fac427aa86" containerName="k8-net-web"
May 15 07:05:53 minikube kubelet[2336]: I0515 07:05:53.291963    2336 memory_manager.go:354] "RemoveStaleState removing state" podUID="1d8e2150-91cf-4701-8b6d-f8fac427aa86" containerName="k8-net-web"
May 15 07:05:53 minikube kubelet[2336]: I0515 07:05:53.429679    2336 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wrvpl\" (UniqueName: \"kubernetes.io/projected/c783c68e-13c3-4924-a633-264c9ca9f607-kube-api-access-wrvpl\") pod \"k8-net-web-674678d67d-j6rvn\" (UID: \"c783c68e-13c3-4924-a633-264c9ca9f607\") " pod="default/k8-net-web-674678d67d-j6rvn"
May 15 07:05:54 minikube kubelet[2336]: I0515 07:05:54.067763    2336 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d359ed2a059898dbe6ce65d25507c4a786ff20255d040c7ce61c3858bb942cfc"
May 15 07:05:57 minikube kubelet[2336]: I0515 07:05:57.160542    2336 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/k8-net-web-674678d67d-j6rvn" podStartSLOduration=1.6429596979999999 podStartE2EDuration="4.160381879s" podCreationTimestamp="2024-05-15 07:05:53 +0000 UTC" firstStartedPulling="2024-05-15 07:05:54.158672361 +0000 UTC m=+13460.785841254" lastFinishedPulling="2024-05-15 07:05:56.676094547 +0000 UTC m=+13463.303263435" observedRunningTime="2024-05-15 07:05:57.130839412 +0000 UTC m=+13463.758008309" watchObservedRunningTime="2024-05-15 07:05:57.160381879 +0000 UTC m=+13463.787550767"
May 15 07:05:57 minikube kubelet[2336]: I0515 07:05:57.667930    2336 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rxkhd\" (UniqueName: \"kubernetes.io/projected/71442dd9-1bc0-497f-92b2-b6084b681ef3-kube-api-access-rxkhd\") pod \"71442dd9-1bc0-497f-92b2-b6084b681ef3\" (UID: \"71442dd9-1bc0-497f-92b2-b6084b681ef3\") "
May 15 07:05:57 minikube kubelet[2336]: I0515 07:05:57.670962    2336 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/71442dd9-1bc0-497f-92b2-b6084b681ef3-kube-api-access-rxkhd" (OuterVolumeSpecName: "kube-api-access-rxkhd") pod "71442dd9-1bc0-497f-92b2-b6084b681ef3" (UID: "71442dd9-1bc0-497f-92b2-b6084b681ef3"). InnerVolumeSpecName "kube-api-access-rxkhd". PluginName "kubernetes.io/projected", VolumeGidValue ""
May 15 07:05:57 minikube kubelet[2336]: I0515 07:05:57.768873    2336 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-rxkhd\" (UniqueName: \"kubernetes.io/projected/71442dd9-1bc0-497f-92b2-b6084b681ef3-kube-api-access-rxkhd\") on node \"minikube\" DevicePath \"\""
May 15 07:05:58 minikube kubelet[2336]: I0515 07:05:58.133961    2336 scope.go:117] "RemoveContainer" containerID="8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc"
May 15 07:05:58 minikube kubelet[2336]: I0515 07:05:58.183660    2336 scope.go:117] "RemoveContainer" containerID="8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc"
May 15 07:05:58 minikube kubelet[2336]: E0515 07:05:58.185977    2336 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc" containerID="8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc"
May 15 07:05:58 minikube kubelet[2336]: I0515 07:05:58.186078    2336 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc"} err="failed to get container status \"8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc\": rpc error: code = Unknown desc = Error response from daemon: No such container: 8795e7682e3500c67e8da0db1f432603282d3dae320b42fa1b9abd04d3cbf1cc"
May 15 07:05:59 minikube kubelet[2336]: I0515 07:05:59.815214    2336 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="71442dd9-1bc0-497f-92b2-b6084b681ef3" path="/var/lib/kubelet/pods/71442dd9-1bc0-497f-92b2-b6084b681ef3/volumes"


==> kubernetes-dashboard [b6d51eca253d] <==
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/15 07:21:57 Getting list of namespaces
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn request from 127.0.0.1: 
2024/05/15 07:21:57 Getting details of k8-net-web-674678d67d-j6rvn pod in default namespace
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/05/15 07:21:57 Getting events related to a pod in namespace
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/15 07:21:57 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:21:57 received 0 resources from sidecar instead of 1
2024/05/15 07:21:57 received 0 resources from sidecar instead of 1
2024/05/15 07:21:57 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:21:57 [2024-05-15T07:21:57Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/15 07:22:02 Getting list of namespaces
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn request from 127.0.0.1: 
2024/05/15 07:22:02 Getting details of k8-net-web-674678d67d-j6rvn pod in default namespace
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/05/15 07:22:02 Getting events related to a pod in namespace
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/15 07:22:02 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:02 received 0 resources from sidecar instead of 1
2024/05/15 07:22:02 received 0 resources from sidecar instead of 1
2024/05/15 07:22:02 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:02 [2024-05-15T07:22:02Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/15 07:22:07 Getting list of namespaces
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn request from 127.0.0.1: 
2024/05/15 07:22:07 Getting details of k8-net-web-674678d67d-j6rvn pod in default namespace
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/05/15 07:22:07 Getting events related to a pod in namespace
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/15 07:22:07 received 0 resources from sidecar instead of 1
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:07 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:07 received 0 resources from sidecar instead of 1
2024/05/15 07:22:07 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:07 [2024-05-15T07:22:07Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/05/15 07:22:09 Getting list of namespaces
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn request from 127.0.0.1: 
2024/05/15 07:22:09 Getting details of k8-net-web-674678d67d-j6rvn pod in default namespace
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/05/15 07:22:09 Getting events related to a pod in namespace
2024/05/15 07:22:09 received 0 resources from sidecar instead of 1
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:09 received 0 resources from sidecar instead of 1
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Incoming HTTP/1.1 GET /api/v1/pod/default/k8-net-web-674678d67d-j6rvn/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/05/15 07:22:09 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Outcoming response to 127.0.0.1 with 200 status code
2024/05/15 07:22:09 No persistentvolumeclaims found related to k8-net-web-674678d67d-j6rvn pod
2024/05/15 07:22:09 [2024-05-15T07:22:09Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [1cfdbde7ffa8] <==
I0515 02:34:00.463364       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0515 02:34:00.475654       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0515 02:34:00.475819       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0515 02:34:00.491766       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0515 02:34:00.491996       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e387056c-06be-4287-90bd-6db22bedd597!
I0515 02:34:00.492938       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"05adcca8-e30d-449d-82e7-8b7cc26cbdb7", APIVersion:"v1", ResourceVersion:"427", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e387056c-06be-4287-90bd-6db22bedd597 became leader
I0515 02:34:00.593152       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e387056c-06be-4287-90bd-6db22bedd597!


==> storage-provisioner [5a0168b56bc3] <==
I0515 02:33:38.347269       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0515 02:33:59.417600       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

